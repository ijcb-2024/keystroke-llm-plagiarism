{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_curve\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import sklearn\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import gc \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global Variables\n",
    "'''\n",
    "M : Length of each sequence\n",
    "'''\n",
    "\n",
    "M = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Root Dataset Directory\n",
    "\n",
    "ROOT = \"Dataset/Raw_Temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(ROOT,'fixed_data.txt'),'r') as f :\n",
    "  fixed_data = json.load(f)\n",
    "\n",
    "with open(os.path.join(ROOT,'free_data.txt'),'r') as f :\n",
    "  free_data = json.load(f)\n",
    "\n",
    "with open(os.path.join(ROOT,'Raw_Temp_Gay_Marriage_Fixed.json')) as f :\n",
    "  gay_marriage_fixed = json.load(f)\n",
    "\n",
    "with open(os.path.join(ROOT,'Raw_Temp_Gay_Marriage_Free.json')) as f :\n",
    "  gay_marriage_free = json.load(f)\n",
    "\n",
    "with open(os.path.join(ROOT,'Raw_Temp_Gun_Control_Fixed.json')) as f :\n",
    "  gun_control_fixed = json.load(f)\n",
    "\n",
    "with open(os.path.join(ROOT,'Raw_Temp_Gun_Control_Free.json')) as f :\n",
    "  gun_control_free = json.load(f)\n",
    "\n",
    "with open(os.path.join(ROOT,'Raw_Temp_Restaurant_Review_Fixed.json')) as f :\n",
    "  rest_fixed = json.load(f)\n",
    "\n",
    "with open(os.path.join(ROOT,'Raw_Temp_Restaurant_Review_Free.json')) as f :\n",
    "  rest_free = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ROOT,'Buffalo_Fixed.json')) as f :\n",
    "  buffalo_fixed = json.load(f)\n",
    "  \n",
    "with open(os.path.join(ROOT,'Buffalo_Free.json')) as f :\n",
    "  buffalo_free = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We divide each user's keystroke into batches of size 15.\n",
    "\n",
    "def divide_into_batches(x) :\n",
    "  num = len(x) //M\n",
    "  list_text = []\n",
    "\n",
    "  for i in range(num):\n",
    "    temp = x[int(i*len(x)/num):min(int((i+1)*len(x)/num),len(x))]\n",
    "    \n",
    "    if len(temp) < 0.8*M :\n",
    "      continue\n",
    "      print(\"Too Small\",len(temp))\n",
    "    \n",
    "    list_text.append(temp)\n",
    "\n",
    "  return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_buffalo(data,verbose=0) :\n",
    "  for key in data.keys() :\n",
    "    if verbose == 1 :\n",
    "      data[key] = json.loads(data[key])\n",
    "\n",
    "    timestamp_kd = []\n",
    "    timestamp_ku = []\n",
    "    list_ = data[key][\"keyboard_data\"]\n",
    "    \n",
    "    for i in range(len(data[key][\"keyboard_data\"])) :\n",
    "      if list_[i][1].lower() == \"kd\" :\n",
    "        timestamp_kd.append(int(list_[i][2]) - int(list_[i-1][2]) if i > 0 else int(list_[i][2]))\n",
    "      else :\n",
    "        timestamp_ku.append(int(list_[i][2]) - int(list_[i-1][2]) if i > 0 else int(list_[i][2]))\n",
    "      \n",
    "    timestamp_kd = np.array(timestamp_kd, dtype=np.float32)\n",
    "    timestamp_ku = np.array(timestamp_ku, dtype=np.float32)\n",
    "    \n",
    "    if len(timestamp_kd) > 0 :\n",
    "      ## Min Max Scaling\n",
    "      timestamp_kd = (timestamp_kd - min(timestamp_kd))/(max(timestamp_kd) - min(timestamp_kd))\n",
    "    \n",
    "    if len(timestamp_ku) > 0 :\n",
    "      timestamp_ku = (timestamp_ku - min(timestamp_ku))/(max(timestamp_ku) - min(timestamp_ku))\n",
    "    \n",
    "    ku_count = 0\n",
    "    kd_count = 0\n",
    "    \n",
    "    for i in range(len(data[key][\"keyboard_data\"])) :\n",
    "      ## Swap 0 element with 1 element\n",
    "      temp = list_[i][0]\n",
    "      list_[i][0] = list_[i][1]\n",
    "      list_[i][1] = temp\n",
    "      \n",
    "      if list_[i][0].lower() == \"ku\" :\n",
    "        list_[i][2] = float(timestamp_ku[ku_count])\n",
    "        ku_count += 1\n",
    "      else :\n",
    "        list_[i][2] = float(timestamp_kd[kd_count])\n",
    "        kd_count += 1\n",
    "\n",
    "    data[key][\"keyboard_data\"] = list_[1:]\n",
    "    data[key] = divide_into_batches(data[key][\"keyboard_data\"])\n",
    "\n",
    "  return data\n",
    "\n",
    "buffalo_free = process_data_buffalo(buffalo_free)\n",
    "buffalo_fixed = process_data_buffalo(buffalo_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data,verbose=0) :\n",
    "  for key in data.keys() :\n",
    "    if verbose == 1 :\n",
    "      data[key] = json.loads(data[key])\n",
    "\n",
    "    timestamp_kd = []\n",
    "    timestamp_ku = []\n",
    "    list_ = data[key][\"keyboard_data\"]\n",
    "    \n",
    "    for i in range(len(data[key][\"keyboard_data\"])) :\n",
    "      if list_[i][0] == \"KD\" :\n",
    "        timestamp_kd.append(list_[i][2] - list_[i-1][2] if i > 0 else list_[i][2])\n",
    "      else :\n",
    "        timestamp_ku.append(list_[i][2] - list_[i-1][2] if i > 0 else list_[i][2])\n",
    "      \n",
    "    timestamp_kd = np.array(timestamp_kd, dtype=np.float32)\n",
    "    timestamp_ku = np.array(timestamp_ku, dtype=np.float32)\n",
    "    \n",
    "    if len(timestamp_kd) > 0 :\n",
    "      ## Min Max Scaling\n",
    "      timestamp_kd = (timestamp_kd - min(timestamp_kd))/(max(timestamp_kd) - min(timestamp_kd))\n",
    "    \n",
    "    if len(timestamp_ku) > 0 :\n",
    "      timestamp_ku = (timestamp_ku - min(timestamp_ku))/(max(timestamp_ku) - min(timestamp_ku))\n",
    "    \n",
    "    ku_count = 0\n",
    "    kd_count = 0\n",
    "    \n",
    "    for i in range(len(data[key][\"keyboard_data\"])) :\n",
    "      if list_[i][0] == \"KU\" :\n",
    "        list_[i][2] = float(timestamp_ku[ku_count])\n",
    "        ku_count += 1\n",
    "      else :\n",
    "        list_[i][2] = float(timestamp_kd[kd_count])\n",
    "        kd_count += 1\n",
    "\n",
    "    data[key][\"keyboard_data\"] = list_[1:]\n",
    "    data[key] = divide_into_batches(data[key][\"keyboard_data\"])\n",
    "\n",
    "  return data\n",
    "\n",
    "free_data = process_data(buffalo_free)\n",
    "fixed_data = process_data(buffalo_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into training and testing data based on keyboard id\n",
    "key_0_free = {}\n",
    "key_1_free = {}\n",
    "key_2_free = {}\n",
    "key_3_free = {}\n",
    "\n",
    "key_0_fixed = {}\n",
    "key_1_fixed = {}\n",
    "key_2_fixed = {}\n",
    "key_3_fixed = {}\n",
    "\n",
    "for key in free_data.keys() :\n",
    "    new_key = key[:4]\n",
    "    \n",
    "    if key[4] == '0' :\n",
    "        key_0_free[new_key] = free_data[key]\n",
    "    \n",
    "    elif key[4] == '1' : \n",
    "        key_1_free[new_key] = free_data[key]\n",
    "\n",
    "    elif key[4] == '2' :\n",
    "        key_2_free[new_key] = free_data[key]\n",
    "        \n",
    "    elif key[4] == '3' :\n",
    "        key_3_free[new_key] = free_data[key]\n",
    "        \n",
    "for key in fixed_data.keys() :\n",
    "    new_key = key[:4]\n",
    "    \n",
    "    if key[4] == '0' :\n",
    "        key_0_fixed[new_key] = fixed_data[key]\n",
    "    \n",
    "    elif key[4] == '1' : \n",
    "        key_1_fixed[new_key] = fixed_data[key]\n",
    "\n",
    "    elif key[4] == '2' :\n",
    "        key_2_fixed[new_key] = fixed_data[key]\n",
    "        \n",
    "    elif key[4] == '3' :\n",
    "        key_3_fixed[new_key] = fixed_data[key]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Dataset Formation & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(adder, addee):\n",
    "  \"\"\"\n",
    "  Updates a dictionary by adding the values from another dictionary.\n",
    "\n",
    "  Parameters:\n",
    "  - adder (dict): The dictionary to be updated.\n",
    "  - addee (dict): The dictionary whose values will be added to the `adder` dictionary.\n",
    "\n",
    "  Returns:\n",
    "  - dict: The updated dictionary.\n",
    "  \"\"\"\n",
    "  \n",
    "  for key in addee:\n",
    "    if key in adder:\n",
    "      adder[key].extend(addee[key])\n",
    "    else:\n",
    "      adder[key] = addee[key]\n",
    "\n",
    "  return adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We can create any combination of datasets for training and \n",
    "    testing in this pipeline to create the training and testing sets.\n",
    "    \n",
    "    key_0_free : Keyboard - 0 Free Data\n",
    "    key_1_free : Keyboard - 1 Free Data\n",
    "    key_2_free : Keyboard - 2 Free Data\n",
    "    key_3_free : Keyboard - 3 Free Data\n",
    "    \n",
    "    key_0_fixed : Keyboard - 0 Fixed Data\n",
    "    key_1_fixed : Keyboard - 1 Fixed Data\n",
    "    key_2_fixed : Keyboard - 2 Fixed Data\n",
    "    key_3_fixed : Keyboard - 3 Fixed Data\n",
    "'''\n",
    "\n",
    "fixed_data_test = {}\n",
    "free_data_test = {}\n",
    "\n",
    "fixed_data_test.update(key_0_fixed)\n",
    "free_data_test.update(key_0_free)\n",
    "\n",
    "fixed_data_train = {}\n",
    "free_data_train = {}\n",
    "\n",
    "fixed_data_train.update(key_3_fixed)\n",
    "fixed_data_train.update(key_2_fixed)\n",
    "fixed_data_train.update(key_1_fixed)\n",
    "free_data_train.update(key_3_free)\n",
    "free_data_train.update(key_2_free)\n",
    "free_data_train.update(key_1_free)\n",
    "\n",
    "## For specific case combine the datasets into the training set only. Leave the test set empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate data based on keyboard type\n",
    "\n",
    "##Key board map to map each key to a number\n",
    "keyboard_map = {\n",
    "    'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10,\n",
    "    'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20,\n",
    "    'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26,\n",
    "    '0': 27, 'd0' : 27, 'd1' : 28, '1': 28, '2': 29, 'd2': 29, '3': 30, 'd3': 30, '4': 31, 'd4' : 31,'5': 32, 'd5':32, '6': 33, 'd6':33, '7': 34, 'd7': 34, '8': 35, 'd8': 35, '9': 36, 'd9':36,\n",
    "    'f1': 37, 'f2': 38, 'f3': 39, 'f4': 40, 'f5': 41, 'f6': 42, 'f7': 43, 'f8': 44, 'f9': 45, 'f10': 46,\n",
    "    'f11': 47, 'f12': 48, 'esc': 49, '`': 50, '-': 51, 'subtract':51, '=': 52, 'backspace': 53,'back':53, 'tab': 54, '[': 55, ']': 56,\n",
    "    '\\\\': 57, 'capslock': 58,'capital':58, ';': 59, '\\'': 60, 'enter': 61, 'return':61, 'shift': 62, ',': 63, '.': 64, 'decimal':64, 'oemperiod': 64 , '/': 65 ,'divide':65, 'control': 66,'ctrl':66,\n",
    "    'alt': 67, ' ': 68, 'printscreen': 69, 'scrolllock': 70,'scroll':70, 'pause': 71, 'insert': 72, 'home': 73, 'pageup': 74,\n",
    "    'delete': 75, 'end': 76, 'pagedown': 77, 'arrowup': 78, 'arrowleft': 79, 'arrowdown': 80, 'arrowright': 81,\n",
    "    'numlock': 82, 'numpad0': 83, 'numpad1': 84, 'numpad2': 85, 'numpad3': 86, 'numpad4': 87, 'numpad5': 88,\n",
    "    'numpad6': 89, 'numpad7': 90, 'numpad8': 91, 'numpad9': 92, 'numpadmultiply': 93, 'numpadadd': 94,\n",
    "    'numpadsubtract': 95, 'numpaddecimal': 96, 'numpaddivide': 97, 'numpadenter': 98, 'contextmenu': 99,\n",
    "    'leftctrl': 100, 'leftshift': 101, 'leftshiftkey':101, 'leftalt': 102, 'lmenu':102, 'leftmeta': 103, 'rightctrl': 104,'rcontrolkey':104, 'rightshift': 105, 'rshiftkey': 105,\n",
    "    'rightalt': 106, 'rmenu':106, 'rightmeta': 107, ':': 108, 'colon': 108, 'unidentified':0, ')': 109, '(':110, 'meta':111, '≠':112, '@':113, '>':114,'<':115, '*':116,'+':117,'add' : 117,'#':118,'$':119, '\"':120, 'process':121,'_':122,\n",
    "    '{':123,'}':124,'?':134,'f1':135,'f2':136,'f3':137,'f4':138,'f5':139,'f6':140,'f7':141,'f14':142,'´':143,'':0,'©':144,'escape':49,'clear':145,'lcontrolkey':100,'lshiftkey':101, 'space':68, 'left': 146, 'right': 147, 'up':148, 'down': 149, 'apps':150,\n",
    "    'rwin' : 151, 'next': 152, 'lwin' : 153, 'browserback':154, 'browserforward':155, 'browserrefresh':156, 'browserstop':157, 'browsersearch':158, 'browserfavorites':159, 'browserhome':160, 'volumemute':161, 'volumedown':162, 'volumeup':163, 'medianexttrack':164, 'mediaprevioustrack':165\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace the character with the ascii value, KD : 0, KU : 1, timestamp : relative\n",
    "def convert_list(list_) :\n",
    "  \"\"\"\n",
    "  Converts a list of key events into a transformed list.\n",
    "\n",
    "  Args:\n",
    "    list_ (list): A list of key events, where each event is represented as a tuple of three elements: \n",
    "                  the key action ('KD' for Key Down or 'KU' for Key Up), the key value, and the timestamp.\n",
    "\n",
    "  Returns:\n",
    "    list: A transformed list where each event is represented as a list with the following elements:\n",
    "          - 0 for Key Down or 1 for Key Up\n",
    "          - The normalized value of the key (if applicable)\n",
    "          - The timestamp difference between the current event and the previous event\n",
    "\n",
    "  \"\"\"\n",
    "  start_time = 0\n",
    "  trans_list = []\n",
    "  timestamp_list_kd = []\n",
    "  timestamp_list_ku = []\n",
    "  shift_count = 0\n",
    "\n",
    "  for i in range(len(list_)) :\n",
    "    temp = []\n",
    "\n",
    "    ## Assigning value to Key Up and Key Down\n",
    "    if list_[i][0] == 'KD' :\n",
    "      temp.append(0)\n",
    "    else:\n",
    "      temp.append(1)\n",
    "\n",
    "    ## Convert to ascii value\n",
    "    if (len(str(list_[i][1]).lower()) > 1 and isinstance(list_[i][1],int)) or list_[i][1].lower().find(\"oem\") != -1 or list_[i][1].lower().find(\"lbutton,\") != -1:\n",
    "      continue\n",
    "    else :\n",
    "      temp.append(keyboard_map[str(list_[i][1]).lower()]/255)\n",
    "      \n",
    "    if str(list_[i][1]).lower() == 'shift' :\n",
    "      shift_count += 1\n",
    "\n",
    "    ## Store the diff in timestamp\n",
    "    if i>=0 :\n",
    "      temp.append(float(list_[i][2]))\n",
    "  \n",
    "    if shift_count < len(list_)*0.2 :    \n",
    "      trans_list.append(temp)\n",
    "\n",
    "  return trans_list\n",
    "\n",
    "def convert_data(data):\n",
    "  \"\"\"\n",
    "  Convert the given data dictionary into a new dictionary with converted lists.\n",
    "\n",
    "  Args:\n",
    "    data (dict): The input data dictionary.\n",
    "\n",
    "  Returns:\n",
    "    dict: The converted data dictionary.\n",
    "\n",
    "  \"\"\"\n",
    "  p_data = {}\n",
    "\n",
    "  for key in data.keys():\n",
    "    list_compiled = []\n",
    "\n",
    "    for list_ in data[key]:\n",
    "      temp = convert_list(list_)\n",
    "\n",
    "      if len(temp) > 0:\n",
    "        list_compiled.append(temp)\n",
    "\n",
    "    if len(list_compiled) > 0:\n",
    "      p_data[key] = list_compiled\n",
    "\n",
    "  return p_data\n",
    "\n",
    "\n",
    "## Converting data\n",
    "free_data_train = convert_data(free_data_train)\n",
    "fixed_data_train = convert_data(fixed_data_train)\n",
    "free_data_test = convert_data(free_data_test)\n",
    "fixed_data_test = convert_data(fixed_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create validation set\n",
    "free_data_val = {}\n",
    "fixed_data_val = {}\n",
    "\n",
    "for key in free_data_train.keys() :\n",
    "    if len(free_data_train[key]) > 1 :\n",
    "        val_sect = int(0.1*len(free_data_train[key]))\n",
    "        free_data_val[key] = free_data_train[key][:val_sect]\n",
    "        free_data_train[key] = free_data_train[key][3*val_sect:]\n",
    "        \n",
    "        \n",
    "for key in fixed_data_train.keys() :\n",
    "    if len(fixed_data_train[key]) > 1 :\n",
    "        fixed_data_val[key] = fixed_data_train[key][:int(0.1*len(fixed_data_train[key]))]       \n",
    "        fixed_data_train[key] = fixed_data_train[key][int(0.3*len(fixed_data_train[key])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding and clipping sequences\n",
    "\n",
    "mask_free_train = {}\n",
    "mask_fixed_train = {}\n",
    "mask_fixed_val = {}\n",
    "mask_free_val = {}\n",
    "mask_free_test = {}\n",
    "mask_fixed_test = {}\n",
    "\n",
    "def pad_clip_seq(x) :\n",
    "  # print(x.shape)\n",
    "  curr_mask = [1]*len(x)\n",
    "\n",
    "  if(len(x) > M) :\n",
    "    ## If length is greater than the sequence length M : Clip the sequence\n",
    "    \n",
    "    x = x[:M]\n",
    "    curr_mask = curr_mask[:M]\n",
    "\n",
    "  ## If length is less than the sequence length M : Pad the sequence \n",
    "  for i in range(max(0,M-len(x))) :\n",
    "    x.append([-1,-1,-1])\n",
    "    curr_mask.append(0)\n",
    "\n",
    "  return x,curr_mask\n",
    "\n",
    "def return_pad_seq(data):\n",
    "  \"\"\"\n",
    "  Pad sequences in the given data dictionary and return the padded sequences along with the corresponding masks.\n",
    "\n",
    "  Args:\n",
    "    data (dict): A dictionary containing sequences to be padded.\n",
    "\n",
    "  Returns:\n",
    "    tuple: A tuple containing the padded sequences and their corresponding masks.\n",
    "\n",
    "  \"\"\"\n",
    "  mask = {}\n",
    "\n",
    "  for key in data.keys():\n",
    "    new_list = []\n",
    "    mask_l = []\n",
    "\n",
    "    for list_ in data[key]:\n",
    "      list_, curr_mask = pad_clip_seq(list_)\n",
    "\n",
    "      new_list.append(list_)\n",
    "      mask_l.append(curr_mask)\n",
    "\n",
    "    data[key] = new_list\n",
    "    mask[key] = mask_l\n",
    "\n",
    "  return data, mask\n",
    "\n",
    "free_data_train,mask_free_train = return_pad_seq(free_data_train)\n",
    "fixed_data_train,mask_fixed_train = return_pad_seq(fixed_data_train)\n",
    "fixed_data_val,mask_fixed_val = return_pad_seq(fixed_data_val)\n",
    "free_data_val,mask_free_val = return_pad_seq(free_data_val)\n",
    "free_data_test,mask_free_test = return_pad_seq(free_data_test)\n",
    "fixed_data_test,mask_fixed_test = return_pad_seq(fixed_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine Pairs of a given set of fixed and free data\n",
    "def combine_pairs(fixed_data,free_data,mask_fixed,mask_free) :\n",
    "  data = {}\n",
    "  mask = {}\n",
    "  y_data = {}\n",
    "\n",
    "  for key in fixed_data.keys() :\n",
    "    if key not in free_data.keys() :\n",
    "      continue\n",
    "    else: \n",
    "      data[key] = []\n",
    "      mask[key] = []\n",
    "      y_data[key] = []\n",
    "\n",
    "      ## For each user, we create pairs of fixed and free data, Label : 1\n",
    "      for fixed_index in range(len(fixed_data[key])) :\n",
    "        for free_index in range(len(free_data[key])) :\n",
    "          data[key].append([fixed_data[key][fixed_index],free_data[key][free_index]])\n",
    "          mask[key].append([mask_fixed[key][fixed_index],mask_free[key][free_index]])\n",
    "          y_data[key].append(1)\n",
    "\n",
    "        for fixed_index_2 in range(len(fixed_data[key])) :\n",
    "          if fixed_data[key][fixed_index_2] == fixed_data[key][fixed_index] or ([fixed_data[key][fixed_index_2],fixed_data[key][fixed_index]] in data[key]) or ([fixed_data[key][fixed_index],fixed_data[key][fixed_index_2]] in data[key]):\n",
    "            continue\n",
    "          else :\n",
    "            data[key].append([fixed_data[key][fixed_index],fixed_data[key][fixed_index_2]])\n",
    "            mask[key].append([mask_fixed[key][fixed_index],mask_fixed[key][fixed_index_2]])\n",
    "            y_data[key].append(0)\n",
    "      \n",
    "      ## For each user, we create pairs of fixed and free data, Label : 0\n",
    "      for free_index in range(len(free_data[key])) :\n",
    "        for free_index_2 in range(len(free_data[key])) :\n",
    "          if free_data[key][free_index_2] == free_data[key][free_index] or ([free_data[key][free_index_2],free_data[key][free_index]] in data[key]) or ([free_data[key][free_index],free_data[key][free_index_2]] in data[key]):\n",
    "            continue\n",
    "          else :\n",
    "            data[key].append([free_data[key][free_index],free_data[key][free_index_2]])\n",
    "            mask[key].append([mask_free[key][free_index],mask_free[key][free_index_2]])\n",
    "            y_data[key].append(0)\n",
    "\n",
    "  return data,y_data,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_train,y_data_train,mask_train = combine_pairs(fixed_data_train,free_data_train,mask_fixed_train,mask_free_train)\n",
    "new_data_val,y_data_val,mask_val = combine_pairs(fixed_data_val,free_data_val,mask_fixed_val,mask_free_val)\n",
    "new_data_test,y_data_test,mask_test = combine_pairs(fixed_data_test,free_data_test,mask_fixed_test,mask_free_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the data into a single list\n",
    "\n",
    "def combine(new_data, mask, y_data):\n",
    "  \"\"\"\n",
    "  Combines the data, mask, and y_data into lists.\n",
    "\n",
    "  Args:\n",
    "    new_data (dict): A dictionary containing the new data.\n",
    "    mask (dict): A dictionary containing the mask.\n",
    "    y_data (dict): A dictionary containing the y_data.\n",
    "\n",
    "  Returns:\n",
    "    tuple: A tuple containing the combined data list, combined mask list, and y_data list.\n",
    "  \"\"\"\n",
    "  combined_data_list = []\n",
    "  combined_mask_list = []\n",
    "  y_data_list = []\n",
    "\n",
    "  for key in new_data.keys():\n",
    "    if len(combined_data_list) == 0:\n",
    "      combined_data_list = new_data[key]\n",
    "      combined_mask_list = mask[key]\n",
    "      y_data_list = y_data[key]\n",
    "    else:\n",
    "      combined_data_list.extend(new_data[key])\n",
    "      combined_mask_list.extend(mask[key])\n",
    "      y_data_list.extend(y_data[key])\n",
    "\n",
    "  return combined_data_list, combined_mask_list, y_data_list\n",
    "\n",
    "combined_data_list_train,combined_mask_list_train,y_data_list_train = combine(new_data_train,mask_train,y_data_train)\n",
    "combined_data_list_val,combined_mask_list_val,y_data_list_val = combine(new_data_val,mask_val,y_data_val)\n",
    "combined_data_list_test,combined_mask_list_test,y_data_list_test = combine(new_data_test,mask_test,y_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([180524, 179535]))\n"
     ]
    }
   ],
   "source": [
    "## Number of unique samples in training set and their distribution\n",
    "\n",
    "len(combined_data_list_train)\n",
    "print(np.unique(y_data_list_train,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Balancing for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = np.arange(len(combined_data_list_train))\n",
    "y_data_list_train = np.array(y_data_list_train)\n",
    "class_0_index = indexes[y_data_list_train == 0]\n",
    "class_1_index = indexes[y_data_list_train == 1]\n",
    "\n",
    "# print(len(class_1_index))\n",
    "min_length = min(len(class_0_index),len(class_1_index))\n",
    "\n",
    "indexes = np.concatenate((class_0_index[:min_length],class_1_index[:min_length]))\n",
    "random.shuffle(indexes)\n",
    "\n",
    "combined_data_list_train = np.array(combined_data_list_train)\n",
    "combined_mask_list_train = np.array(combined_mask_list_train)\n",
    "y_data_list_train = np.array(y_data_list_train)\n",
    "\n",
    "combined_data_list_train = combined_data_list_train[indexes]\n",
    "combined_mask_list_train = combined_mask_list_train[indexes]\n",
    "y_data_list_train = y_data_list_train[indexes]\n",
    "\n",
    "del indexes,class_0_index,class_1_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([179535, 179535]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_data_list_train,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Data Loader\n",
    "\n",
    "class CustomDataset(Dataset) :\n",
    "  def __init__(self,data,mask,label) :\n",
    "    self.data = data\n",
    "    self.mask = mask\n",
    "    self.label = label\n",
    "\n",
    "  def __getitem__(self,index) :\n",
    "    x = torch.tensor(self.data[index],dtype=torch.float32)\n",
    "    m = torch.tensor(self.mask[index],dtype=torch.float32)\n",
    "\n",
    "    counts = np.unique(m[0,:],return_counts=True)\n",
    "    length1 = counts[1][-1]\n",
    "    counts = np.unique(m[1,:],return_counts=True)\n",
    "    length2 = counts[1][-1]\n",
    "\n",
    "    temp = torch.zeros(2)\n",
    "    temp[self.label[index]] = 1\n",
    "\n",
    "    return x[0,:],x[1,:],self.label[index],length1,length2\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config \n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(combined_data_list_train,combined_mask_list_train,y_data_list_train)\n",
    "val_data = CustomDataset(combined_data_list_val,combined_mask_list_val,y_data_list_val)\n",
    "test_data = CustomDataset(combined_data_list_test,combined_mask_list_test,y_data_list_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(fv1, fv2, y, alpha):\n",
    "    # Move all inputs to the specified device\n",
    "    fv1 = fv1.to(device)\n",
    "    fv2 = fv2.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Element-wise square of the difference\n",
    "    squared_diff = (fv1 - fv2) ** 2\n",
    "\n",
    "    # Summing the squared differences along the last dimension and taking the square root for Euclidean distance\n",
    "    d = torch.sqrt(torch.sum(squared_diff, dim=-1))\n",
    "\n",
    "    # Element-wise maximum\n",
    "    max_part = torch.clamp_min(alpha - d, min=0) ** 2\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = (((1 - y) * d ** 2) / 2) + ((y * max_part) / 2)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TypeNet(nn.Module):\n",
    "    def __init__(self, sequence_length, in_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout):\n",
    "        super(TypeNet, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.lstm1 = nn.LSTM(input_size=in_dim, hidden_size=hidden_dim_1, batch_first=True, dropout=0.2)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_dim_1, hidden_size=hidden_dim_2, batch_first=True, dropout=0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=in_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=hidden_dim_1)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=hidden_dim_2*sequence_length)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=output_dim)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "        self.fc1 = nn.Linear(sequence_length*hidden_dim_2, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Weight initialization\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_uniform_(param.data)\n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_uniform_(param.data)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        x = x.to(device)\n",
    "\n",
    "        x = torch.movedim(x, 2, 1)\n",
    "        out = self.bn1(x)\n",
    "        out = torch.movedim(out, 2, 1)\n",
    "\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, length, batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.lstm1(out)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout_1(out)\n",
    "\n",
    "        out = torch.movedim(out, 2, 1)\n",
    "        out = self.bn2(out)\n",
    "        out = torch.movedim(out, 2, 1)\n",
    "\n",
    "        out_p = nn.utils.rnn.pack_padded_sequence(out, length, batch_first=True, enforce_sorted=False)\n",
    "        out_p, _ = self.lstm2(out_p)\n",
    "        out_p, _ = nn.utils.rnn.pad_packed_sequence(out_p, batch_first=True)\n",
    "\n",
    "        out_p = self.act2(out_p)\n",
    "        out_p = torch.reshape(out_p, (out.shape[0], out.shape[1]*out.shape[2]))\n",
    "        out = self.bn3(out_p)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTypeNet(nn.Module) :\n",
    "  def __init__(self,sequence_length,in_dim,hidden_dim_1,hidden_dim_2,output_dim,dropout) :\n",
    "    super(CustomTypeNet,self).__init__()\n",
    "    self.tn1 = TypeNet(sequence_length,in_dim,hidden_dim_1,hidden_dim_2,output_dim,dropout)\n",
    "    self.tn2 = TypeNet(sequence_length,in_dim,hidden_dim_1,hidden_dim_2,output_dim,dropout)\n",
    "\n",
    "  def forward(self,x1,x2,length1,length2) :\n",
    "    x1 = self.tn1(x1,length1)\n",
    "    x2 = self.tn2(x2,length2)\n",
    "\n",
    "    return x1,x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_eer(d, labels):\n",
    "    # Element-wise square of the difference\n",
    "    with torch.no_grad() :\n",
    "      # squared_diff = (fv1 - fv2) ** 2\n",
    "\n",
    "      # # Summing the squared differences along the last dimension and taking the square root for Euclidean distance\n",
    "      # d = torch.sqrt(torch.sum(squared_diff, dim=1)).detach().cpu().numpy()\n",
    "\n",
    "      # Calculate the False Positive Rates, True Positive Rates, and thresholds\n",
    "      fpr, tpr, thresholds = roc_curve(labels, d, pos_label=1)      \n",
    "\n",
    "      # Handle cases where tpr or fpr contains nan values\n",
    "      tpr = np.nan_to_num(tpr)\n",
    "      fpr = np.nan_to_num(fpr)\n",
    "\n",
    "      # Find the EER\n",
    "      eer_threshold = thresholds[np.argmin(np.absolute((1 - tpr) - fpr))]\n",
    "      eer = fpr[np.argmin(np.absolute((1 - tpr) - fpr))]\n",
    "\n",
    "      return eer, eer_threshold, np.average(fpr), np.average(1-tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Parameters\n",
    "EPOCHS = 200\n",
    "LR = 0.001\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H_%M_%S\")\n",
    "new_path = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nas_mount/avinash_ocr/.conda/envs_dirs/atharva/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "model = CustomTypeNet(M,3,128,128,128,DROPOUT)\n",
    "model.to(device)\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# for param in model.parameters() :\n",
    "#   print(param.device)\n",
    "\n",
    "optimizer = Adam(model.parameters(),lr=LR,weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gsz23e41) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509c815863be4c9c94dece1c123a5a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>f1_test</td><td>▃▁█▅▂▁▆▁▁</td></tr><tr><td>f1_train</td><td>▁▃▅▆▇▇▇██</td></tr><tr><td>f1_val</td><td>▂▁▄▄██▇▇▇</td></tr><tr><td>fnr_test</td><td>█▂▄▁▄▄▆▅▆</td></tr><tr><td>fnr_train</td><td>█▅▄▃▂▂▁▁▁</td></tr><tr><td>fnr_val</td><td>▇█▄▄▁▁▂▃▃</td></tr><tr><td>fpr_test</td><td>█▃▂▁▃▄▆▅▄</td></tr><tr><td>fpr_train</td><td>█▅▄▃▂▂▂▁▁</td></tr><tr><td>fpr_val</td><td>▇▇█▇▃▂▃▁▁</td></tr><tr><td>prec_test</td><td>▂▁█▄▁▁▅▁▁</td></tr><tr><td>prec_train</td><td>▁▁▃▅▆▆▇▇█</td></tr><tr><td>prec_val</td><td>▂▁▃▄▇█▆▄▃</td></tr><tr><td>recall_test</td><td>▇▁▆█▃▃▆▁▃</td></tr><tr><td>recall_train</td><td>▁▇▇██████</td></tr><tr><td>recall_val</td><td>▆▇▅▄▃▁▄▇█</td></tr><tr><td>test_accuracy</td><td>▅▁██▁▂▄▁▂</td></tr><tr><td>test_eer</td><td>█▁▂▂▁▂▄▄▅</td></tr><tr><td>test_f1</td><td>▃▁█▅▂▁▆▁▁</td></tr><tr><td>test_fnr</td><td>█▂▄▁▄▄▆▅▆</td></tr><tr><td>test_fpr</td><td>█▃▂▁▃▄▆▅▄</td></tr><tr><td>test_precision</td><td>▂▁█▄▁▁▅▁▁</td></tr><tr><td>test_recall</td><td>▇▁▆█▃▃▆▁▃</td></tr><tr><td>threshold</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▆▆▇▇██</td></tr><tr><td>train_eer</td><td>█▅▄▃▂▂▁▁▁</td></tr><tr><td>train_f1</td><td>▁▃▅▆▇▇▇██</td></tr><tr><td>train_fnr</td><td>█▅▄▃▂▂▁▁▁</td></tr><tr><td>train_fpr</td><td>█▅▄▃▂▂▂▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁</td></tr><tr><td>train_precision</td><td>▁▁▃▅▆▆▇▇█</td></tr><tr><td>train_recall</td><td>▁▇▇██████</td></tr><tr><td>val_accuracy</td><td>▃▁▄▄██▇▅▅</td></tr><tr><td>val_eer</td><td>▇█▇▆▁▁▃▄▃</td></tr><tr><td>val_f1</td><td>▂▁▄▄██▇▇▇</td></tr><tr><td>val_fnr</td><td>▇█▄▄▁▁▂▃▃</td></tr><tr><td>val_fpr</td><td>▇▇█▇▃▂▃▁▁</td></tr><tr><td>val_loss</td><td>█▆▇█▅▆▅▄▁</td></tr><tr><td>val_precision</td><td>▂▁▃▄▇█▆▄▃</td></tr><tr><td>val_recall</td><td>▆▇▅▄▃▁▄▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>f1_test</td><td>0.01928</td></tr><tr><td>f1_train</td><td>0.79065</td></tr><tr><td>f1_val</td><td>0.82733</td></tr><tr><td>fnr_test</td><td>0.47433</td></tr><tr><td>fnr_train</td><td>0.27829</td></tr><tr><td>fnr_val</td><td>0.20574</td></tr><tr><td>fpr_test</td><td>0.46251</td></tr><tr><td>fpr_train</td><td>0.28002</td></tr><tr><td>fpr_val</td><td>0.33091</td></tr><tr><td>path</td><td>12_54_37</td></tr><tr><td>prec_test</td><td>0.0101</td></tr><tr><td>prec_train</td><td>0.72402</td></tr><tr><td>prec_val</td><td>0.78341</td></tr><tr><td>recall_test</td><td>0.22839</td></tr><tr><td>recall_train</td><td>0.87079</td></tr><tr><td>recall_val</td><td>0.87645</td></tr><tr><td>test_accuracy</td><td>0.50215</td></tr><tr><td>test_eer</td><td>0.00087</td></tr><tr><td>test_f1</td><td>0.01928</td></tr><tr><td>test_fnr</td><td>0.47433</td></tr><tr><td>test_fpr</td><td>0.46251</td></tr><tr><td>test_precision</td><td>0.0101</td></tr><tr><td>test_recall</td><td>0.22839</td></tr><tr><td>threshold</td><td>0.33</td></tr><tr><td>train_accuracy</td><td>0.7694</td></tr><tr><td>train_eer</td><td>0.00041</td></tr><tr><td>train_f1</td><td>0.79065</td></tr><tr><td>train_fnr</td><td>0.27829</td></tr><tr><td>train_fpr</td><td>0.28002</td></tr><tr><td>train_loss</td><td>0.47889</td></tr><tr><td>train_precision</td><td>0.72402</td></tr><tr><td>train_recall</td><td>0.87079</td></tr><tr><td>val_accuracy</td><td>0.78335</td></tr><tr><td>val_eer</td><td>0.00045</td></tr><tr><td>val_f1</td><td>0.82733</td></tr><tr><td>val_fnr</td><td>0.20574</td></tr><tr><td>val_fpr</td><td>0.33091</td></tr><tr><td>val_loss</td><td>0.48684</td></tr><tr><td>val_precision</td><td>0.78341</td></tr><tr><td>val_recall</td><td>0.87645</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-haze-18</strong> at: <a href='https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2/runs/gsz23e41' target=\"_blank\">https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2/runs/gsz23e41</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240531_125437-gsz23e41/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gsz23e41). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d0f9f2da5e4a498bcbc516aea2776e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113080288568097, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nas_mount/debnath/Thesis/Atharv/Pipelines/Classification/TypeNet EER/wandb/run-20240531_133612-zxd91qn7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2/runs/zxd91qn7' target=\"_blank\">eager-dragon-19</a></strong> to <a href='https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2' target=\"_blank\">https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2/runs/zxd91qn7' target=\"_blank\">https://wandb.ai/atharva20038/Content%20Specific%20TypeNet%20EER%20V2/runs/zxd91qn7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "## Replace the Dataset with the combination of the dataset chosen for tracing the model performance\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"Content Specific TypeNet EER V2\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": LR,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"M\": M,\n",
    "        \"Dropout\": DROPOUT,\n",
    "        \"Dataset\": \"RF\",\n",
    "        \"Model Path\": new_path\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"18_52_42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_new_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_model() : \n",
    "  dir_list = os.listdir(os.path.join(os.getcwd(),checkpoint_path))\n",
    "  num_max = 0\n",
    "  \n",
    "  for item in dir_list : \n",
    "    res = [int(i) for i in item if i.isdigit()]\n",
    "    num_max = max(res[0],num_max)\n",
    "    \n",
    "  return str(num_max)\n",
    "    \n",
    "  \n",
    "\n",
    "def load_model() : \n",
    "  print(\"Loading Model Checkpoint ....\")\n",
    "  curr_epoch = load_latest_model()\n",
    "  model_path = \"_model{}.pth\".format(curr_epoch)\n",
    "  print(\"Loading : {}\".format(model_path))\n",
    "  model = torch.load(os.path.join(checkpoint_path,model_path))\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:39<00:00,  7.09it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.06it/s]\n",
      "100%|██████████| 638/638 [01:34<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13_36_12\n",
      "Precision train/val/test: 0.7170316275340491/0.8348717948717949/0.8484135389280921\n",
      "Recall train/val/test: 0.7372044448157741/0.80039331366765/0.6738389516332158\n",
      "F1 train/val/test: 0.7269781199106891/0.8172690763052208/0.7508094129990311\n",
      "EER Train/Val/Test: 0.0005315213596573939/0.0013179846287291323/0.00048738479677669274\n",
      "FPR Train/Val/Test: 0.3642737933465132/0.3402169317215728/0.34045630954005035\n",
      "FNR Train/Val/Test: 0.2908530093661181/0.24216820107864254/0.2523365448053171\n",
      "---------------------------\n",
      "EPOCH - 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:35<00:00,  7.34it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.43it/s]\n",
      "100%|██████████| 638/638 [01:31<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.7154187088374943/0.8631415241057543/0.806167788336597\n",
      "Recall train/val/test: 0.8325451861753975/0.8185840707964602/0.6869043725810972\n",
      "F1 train/val/test: 0.7695507680268133/0.8402725208175624/0.741372391332377\n",
      "EER Train/Val/Test: 0.00044766509339633307/0.0012609743211180549/0.0005037591667368033\n",
      "FPR Train/Val/Test: 0.31961223392967336/0.3631108830957022/0.33733601683022263\n",
      "FNR Train/Val/Test: 0.26243474463458333/0.20611191792354672/0.27724488491340726\n",
      "---------------------------\n",
      "EPOCH - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:41<00:00,  6.93it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.03it/s]\n",
      "100%|██████████| 638/638 [01:32<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.7265296011890018/0.8644346871569704/0.8221680021520388\n",
      "Recall train/val/test: 0.8495056674186092/0.7743362831858407/0.732399982571974\n",
      "F1 train/val/test: 0.783219799773532/0.816908713692946/0.774378241872549\n",
      "EER Train/Val/Test: 0.0004130109140802081/0.0016003640466151561/0.00042950285519294224\n",
      "FPR Train/Val/Test: 0.3069538991533151/0.3571760257180279/0.33457303562475593\n",
      "FNR Train/Val/Test: 0.2492343767435341/0.23499030907460391/0.2529509360710879\n",
      "---------------------------\n",
      "EPOCH - 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:35<00:00,  7.33it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.78it/s]\n",
      "100%|██████████| 638/638 [01:33<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.7385926001092057/0.8374384236453202/0.8528170779917419\n",
      "Recall train/val/test: 0.8513715988525914/0.8357915437561455/0.714629575369391\n",
      "F1 train/val/test: 0.7909822916343238/0.8366141732283464/0.7772968530038153\n",
      "EER Train/Val/Test: 0.00039656491231323626/0.0015873365339922746/0.0004268786592432416\n",
      "FPR Train/Val/Test: 0.3003978490463039/0.3447751850437495/0.33648511435331135\n",
      "FNR Train/Val/Test: 0.24218392822362816/0.23325659459335638/0.23214244114695531\n",
      "---------------------------\n",
      "EPOCH - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:38<00:00,  7.14it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.41it/s]\n",
      "100%|██████████| 638/638 [01:34<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.750679122124724/0.8683236382866208/0.8385070346904746\n",
      "Recall train/val/test: 0.8557997047929373/0.8072763028515241/0.7361867729211613\n",
      "F1 train/val/test: 0.799800109835481/0.836687898089172/0.7836805717782845\n",
      "EER Train/Val/Test: 0.00037988325461033186/0.0012034791937401185/0.0004045110608039949\n",
      "FPR Train/Val/Test: 0.291121595540677/0.32605615372017216/0.32648469081412174\n",
      "FNR Train/Val/Test: 0.23642299517801738/0.23437156788308605/0.24038969370862912\n",
      "---------------------------\n",
      "EPOCH - 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:34<00:00,  7.40it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.08it/s]\n",
      "100%|██████████| 638/638 [01:34<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.756432827403575/0.8643190056965303/0.8059722377314853\n",
      "Recall train/val/test: 0.8591528114295263/0.8205506391347099/0.7226183553703556\n",
      "F1 train/val/test: 0.8045273177728517/0.8418663303909205/0.7616872666449656\n",
      "EER Train/Val/Test: 0.0003700065424028381/0.0021653986779277307/0.0004565640347697842\n",
      "FPR Train/Val/Test: 0.2860663713469839/0.3394943684721094/0.33376934835434136\n",
      "FNR Train/Val/Test: 0.23278918985718727/0.2185924872580716/0.26011221252007255\n",
      "---------------------------\n",
      "EPOCH - 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:34<00:00,  7.40it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.87it/s]\n",
      "100%|██████████| 638/638 [01:33<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.7611263025644815/0.9051383399209486/0.8056147555367698\n",
      "Recall train/val/test: 0.8604561784610243/0.788102261553589/0.7961306443317208\n",
      "F1 train/val/test: 0.8077490196078432/0.8425755584756899/0.8005463968390361\n",
      "EER Train/Val/Test: 0.00036171490942411985/0.001554264072696255/0.0003903677818894055\n",
      "FPR Train/Val/Test: 0.2831958282673315/0.36270136329133357/0.32563456101614313\n",
      "FNR Train/Val/Test: 0.22929969832275146/0.19265845208260207/0.2313000852898007\n",
      "---------------------------\n",
      "EPOCH - 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:35<00:00,  7.34it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.02it/s]\n",
      "100%|██████████| 638/638 [01:35<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.7678628088684409/0.885792349726776/0.75104264235428\n",
      "Recall train/val/test: 0.8626785863480658/0.7969518190757129/0.8107614948777332\n",
      "F1 train/val/test: 0.8125139348282836/0.8390269151138716/0.7794052534932732\n",
      "EER Train/Val/Test: 0.00035567850317759595/0.001554805106543766/0.00042864459975128325\n",
      "FPR Train/Val/Test: 0.2789831653425427/0.3481107097044109/0.3201334274671568\n",
      "FNR Train/Val/Test: 0.22724918384543447/0.21731830576945396/0.23429778245841165\n",
      "---------------------------\n",
      "EPOCH - 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:38<00:00,  7.15it/s]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.96it/s]\n",
      "100%|██████████| 638/638 [01:36<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train/val/test: 0.7731517762215423/0.8665644171779141/0.8101667979833486\n",
      "Recall train/val/test: 0.8627509956275935/0.8333333333333334/0.7951928916804543\n",
      "F1 train/val/test: 0.8154976887194769/0.849624060150376/0.8023108734598179\n",
      "EER Train/Val/Test: 0.00034999952787164834/0.0021719758981473356/0.0003957968967819832\n",
      "FPR Train/Val/Test: 0.27624314076220263/0.34957917955236784/0.3198601442574365\n",
      "FNR Train/Val/Test: 0.22358433514533751/0.21879570537664864/0.21043320258830517\n",
      "---------------------------\n",
      "EPOCH - 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702/702 [01:35<00:00,  7.32it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.32it/s]\n",
      "100%|██████████| 638/638 [01:33<00:00,  6.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from torch.nn.functional import cosine_similarity, binary_cross_entropy\n",
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "loss_plot = []\n",
    "val_loss_plot = []\n",
    "train_eer_loss = []\n",
    "val_eer_loss = []\n",
    "iter = []\n",
    "path = None\n",
    "curr_epoch = 0\n",
    "cos = CosineSimilarity(dim=1)\n",
    "\n",
    "## Loading model from checkpoint\n",
    "if is_new_model : \n",
    "  path = None\n",
    "else: \n",
    "  path = checkpoint_path\n",
    "  model = load_model()\n",
    "  curr_epoch = int(load_latest_model()) + 1\n",
    "\n",
    "\n",
    "for epoch in range(curr_epoch,EPOCHS+curr_epoch) :\n",
    "  train_loss = 0\n",
    "  val_loss = 0\n",
    "  train_count = 0\n",
    "  val_count = 0\n",
    "  acc_train_avg = 0\n",
    "  acc_val_avg = 0\n",
    "  f1_train = 0\n",
    "  f1_val = 0\n",
    "  predicted_train = []\n",
    "  label_train = []\n",
    "  predicted_val = []\n",
    "  label_val = []\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  eer_train = 0\n",
    "  eer_val = 0\n",
    "  eer_test = 0\n",
    "  \n",
    "  fpr_train = 0\n",
    "  fpr_val = 0\n",
    "  fpr_test = 0\n",
    "  \n",
    "  fnr_train = 0\n",
    "  fnr_val = 0\n",
    "  fnr_test = 0\n",
    "\n",
    "  print(\"EPOCH - {}\".format(epoch))\n",
    "  \n",
    "  ### Training Loop ###\n",
    "  \n",
    "  for item in tqdm(train_loader) :\n",
    "    ## Train Model \n",
    "    try: \n",
    "      model.train()\n",
    "      x1,x2,label,length1,length2 = item\n",
    "      x1 = x1.to(device)\n",
    "      x2 = x2.to(device)\n",
    "      label = label.to(device)\n",
    "      label = label.float()\n",
    "      optimizer.zero_grad()\n",
    "      output = model(x1,x2,length1,length2)\n",
    "      d = (cos(output[0],output[1]) + 1)/2\n",
    "      loss_ = loss(d,label)\n",
    "      eer_result = calculate_eer(d.detach().cpu().numpy(),np.array(label.cpu()))\n",
    "      eer_train += eer_result[0]/output[0].shape[0]    \n",
    "      fpr_train += eer_result[2]\n",
    "      fnr_train += eer_result[3]\n",
    "      \n",
    "      train_loss += loss_.item()\n",
    "      loss_.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      curr_pred = (d.detach().cpu().numpy() >= 0.33)*1\n",
    "      curr_label = label.detach().cpu().numpy()\n",
    "      predicted_train.extend(curr_pred)\n",
    "      label_train.extend(curr_label)\n",
    "      \n",
    "      ## Accuracy Calculation\n",
    "      acc_curr = np.sum(curr_pred == curr_label)/output[0].shape[0]\n",
    "      acc_train_avg += acc_curr\n",
    "        \n",
    "      ## Increase Count\n",
    "      train_count += 1\n",
    "      \n",
    "      ## Delete variables\n",
    "      del output,x1,x2,length1,length2,label,loss_,curr_pred,curr_label,d,eer_result\n",
    "      \n",
    "    except Exception as e:\n",
    "      continue\n",
    "    \n",
    "  ### Validation Loop ###\n",
    "\n",
    "  with torch.no_grad() :\n",
    "    try: \n",
    "      for item in tqdm(val_loader) :\n",
    "      ## Testing Model \n",
    "        x1,x2,label,length1,length2 = item\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        label = label.to(device)\n",
    "        label = label.float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x1,x2,length1,length2)\n",
    "        d = (cosine_similarity(output[0],output[1]) + 1)/2\n",
    "        loss_ = loss(d,label)\n",
    "        val_loss += torch.mean(loss_).item()\n",
    "        \n",
    "        eer_result = calculate_eer(d.detach().cpu().numpy(),np.array(label.cpu()))\n",
    "        eer_val += eer_result[0]/output[0].shape[0]    \n",
    "        fpr_val += eer_result[2]\n",
    "        fnr_val += eer_result[3]    \n",
    "\n",
    "        curr_pred = (d.detach().cpu().numpy() >= 0.33)*1\n",
    "        curr_label = label.detach().cpu().numpy()\n",
    "        predicted_val.extend(curr_pred)\n",
    "        label_val.extend(curr_label)\n",
    "        \n",
    "        ## Accuracy Calculation\n",
    "        acc_curr = np.sum(curr_pred == curr_label)/output[0].shape[0]\n",
    "        acc_val_avg += acc_curr\n",
    "        \n",
    "        ## Increase Count\n",
    "        val_count += 1\n",
    "\n",
    "        ## Deleting \n",
    "        del output,x1,x2,length1,length2,label,loss_,curr_pred,curr_label,d,eer_result\n",
    "      \n",
    "    except Exception as e:\n",
    "      continue\n",
    "      \n",
    "\n",
    "    \n",
    "  acc_avg_test = 0\n",
    "  av_f1 = 0\n",
    "  av_prec = 0\n",
    "  av_recall = 0\n",
    "  test_count = 0\n",
    "  \n",
    "  ### Testing Loop ###\n",
    "\n",
    "  with torch.no_grad() :\n",
    "    try : \n",
    "      for item in tqdm(test_loader) :\n",
    "        x1,x2,label,length1,length2 = item\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        label = label.to(device)\n",
    "        label = label.float()\n",
    "        optimizer.zero_grad()\n",
    "        model.eval()\n",
    "        output = model(x1,x2,length1,length2)\n",
    "        d = (cosine_similarity(output[0],output[1]) + 1)/2\n",
    "        loss_ = loss(d,label)\n",
    "        eer_result = calculate_eer(d.detach().cpu().numpy(),np.array(label.cpu()))\n",
    "        eer_test += eer_result[0]/output[0].shape[0]    \n",
    "        fpr_test += eer_result[2]\n",
    "        fnr_test += eer_result[3]\n",
    "      \n",
    "        test_count += 1\n",
    "        \n",
    "        threshold = 0.33\n",
    "        acc_avg_test += np.sum((d.detach().cpu().numpy() >= threshold)*1 == label.detach().cpu().numpy())/output[0].shape[0]\n",
    "        av_f1 += f1_score((d.detach().cpu().numpy() >= threshold)*1,label.detach().cpu().numpy())\n",
    "        av_prec += precision_score((d.detach().cpu().numpy() >= threshold)*1,label.detach().cpu().numpy())\n",
    "        av_recall += recall_score((d.detach().cpu().numpy() >= threshold)*1,label.detach().cpu().numpy())\n",
    "\n",
    "        del output,x1,x2,length1,length2,label,loss_,d,eer_result\n",
    "        \n",
    "    except Exception as e:\n",
    "      continue\n",
    "  \n",
    "  ## Saving Model Checkpoint\n",
    "  if path: \n",
    "    torch.save(model,os.path.join(os.getcwd(),path,'_model{}.pth').format(epoch))\n",
    "  else: \n",
    "    path = new_path\n",
    "    print(new_path)\n",
    "    os.mkdir(os.path.join(os.getcwd(),path))\n",
    "    \n",
    "  ## Calculate f1 score\n",
    "  f1_train = f1_score(label_train,predicted_train)\n",
    "  prec_train = precision_score(label_train,predicted_train)\n",
    "  prec_val = precision_score(label_val,predicted_val)\n",
    "  recall_train = recall_score(label_train,predicted_train)\n",
    "  recall_val = recall_score(label_val,predicted_val)\n",
    "  f1_val = f1_score(label_val,predicted_val)\n",
    "  \n",
    "  print(\"Precision train/val/test: {}/{}/{}\".format(prec_train,prec_val,av_prec/test_count))\n",
    "  print(\"Recall train/val/test: {}/{}/{}\".format(recall_train,recall_val,av_recall/test_count))\n",
    "  print(\"F1 train/val/test: {}/{}/{}\".format(f1_train,f1_val,av_f1/test_count))\n",
    "  print(\"EER Train/Val/Test: {}/{}/{}\".format(eer_train/train_count,eer_val/val_count,eer_test/test_count))\n",
    "  print(\"FPR Train/Val/Test: {}/{}/{}\".format(fpr_train/train_count,fpr_val/val_count,fpr_test/test_count))\n",
    "  print(\"FNR Train/Val/Test: {}/{}/{}\".format(fnr_train/train_count,fnr_val/val_count,fnr_test/test_count))\n",
    "  \n",
    "  ## Logging into wandb\n",
    "  wandb.log({\"train_accuracy\": acc_train_avg/train_count,\"val_accuracy\": acc_val_avg/val_count,\"train_loss\": train_loss/train_count,\"val_loss\": val_loss/val_count,\"train_f1\": f1_train,\"val_f1\": f1_val,\"test_accuracy\":acc_avg_test/test_count,\"test_f1\":av_f1/test_count,\"train_eer\":eer_train/train_count,\"val_eer\":eer_val/val_count,\"test_eer\":eer_test/test_count, \"train_fpr\":fpr_train/train_count,\"val_fpr\":fpr_val/val_count,\"test_fpr\":fpr_test/test_count,\"train_fnr\":fnr_train/train_count,\"val_fnr\":fnr_val/val_count,\"test_fnr\":fnr_test/test_count,\"train_precision\":prec_train,\"val_precision\":prec_val,\"test_precision\":av_prec/test_count,\"train_recall\":recall_train,\"val_recall\":recall_val,\"test_recall\":av_recall/test_count,\"threshold\":threshold,\"epoch\":epoch,\"path\":path,\"f1_train\":f1_train,\"f1_val\":f1_val,\"f1_test\":av_f1/test_count,\"prec_train\":prec_train,\"prec_val\":prec_val,\"prec_test\":av_prec/test_count,\"recall_train\":recall_train,\"recall_val\":recall_val,\"recall_test\":av_recall/test_count,\"fpr_train\":fpr_train/train_count,\"fpr_val\":fpr_val/val_count,\"fpr_test\":fpr_test/test_count,\"fnr_train\":fnr_train/train_count,\"fnr_val\":fnr_val/val_count,\"fnr_test\":fnr_test/test_count})\n",
    "  \n",
    "  ## Appending to list\n",
    "  val_loss_plot.append(val_loss/val_count)\n",
    "  loss_plot.append(train_loss/train_count)\n",
    "  iter.append(epoch)\n",
    "  train_eer_loss.append(acc_train_avg/train_count)\n",
    "  val_eer_loss.append(acc_val_avg/val_count)\n",
    "\n",
    "  print(\"---------------------------\")\n",
    "  gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atharva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
